{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product, chain\n",
    "from rl_parsers.dpomdp import parse\n",
    "\n",
    "# known problem, policy is not stochastic for the case of equal observation at last step\n",
    "\n",
    "class DecTiger():\n",
    "\tdef __init__(self, horizon=3):\n",
    "\t\twith open('dectiger.dpomdp') as f:\n",
    "\t\t\tself.d = parse(f.read())\n",
    "\n",
    "\t\tself.horizon = horizon\n",
    "\t\tself.n_agents = len(self.d.agents)\n",
    "\t\tself.states = self.d.states\n",
    "\t\tself.actions = self.d.actions[0]\n",
    "\t\tself.observations = self.d.observations[0]\n",
    "\t\tself.joint_observations = tuple(\n",
    "\t\t\tproduct(self.observations, repeat=self.n_agents))\n",
    "\t\tself.joint_actions = tuple(product(self.actions, repeat=self.n_agents))\n",
    "\n",
    "\t\t# (state, iteration, history_objects)\n",
    "\t\tself.histories = [[] for _ in self.states]\n",
    "\t\t# history object: ([action-observations], Pr(h|s), cul_reward)\n",
    "\t\tfor s in self.states:\n",
    "\t\t\tself.grow_history(s)\n",
    "\n",
    "\t\tself.state_history_values = [[] for _ in self.states]\n",
    "\t\tfor s in range(len(self.states)):\n",
    "\t\t\tself.gen_state_history_values(s)\n",
    "\n",
    "\t\tself.prob_h = self.get_history_probs()\n",
    "\t\tself.prob_s_given_h = self.get_state_given_hist_probs()\n",
    "\n",
    "\t\tself.state_values = self.gen_state_values()\n",
    "\t\tself.history_values = self.gen_history_values()\n",
    "\n",
    "\tdef transition(self, state, actions):\n",
    "\t\ta = self.actions2a(actions)\n",
    "\t\ts = self.states.index(state)\n",
    "\t\tobs_probs = [self.d.O[(*a, s, *self.obs2o(obs))]\n",
    "                    for obs in self.joint_observations]\n",
    "\t\treward = [float(self.d.R[(*a, s, s, *self.obs2o(obs))])\n",
    "                    for obs in self.joint_observations]\n",
    "\t\treturn obs_probs, reward\n",
    "\n",
    "\tdef actions2a(self, actions):\n",
    "\t\treturn [self.actions.index(action) for action in actions]\n",
    "\n",
    "\tdef obs2o(self, observations):\n",
    "\t\treturn [self.observations.index(obs) for obs in observations]\n",
    "\n",
    "\tdef grow_history(self, state, histories=None, horizon=None):\n",
    "\t\t# dectiger states are self-transitioning, so we do not consider state transitions\n",
    "\t\t# history will be associated with a probability in a tuple,\n",
    "\t\t# and branch out according to the transition table\n",
    "\t\t# default to empty history set with the only empty history with probability 1 and zero reward\n",
    "\t\tif histories is None:\n",
    "\t\t\thistories = [([], 1, 0)]\n",
    "\t\tif horizon is None:\n",
    "\t\t\thorizon = self.horizon\n",
    "\t\tif horizon <= 0:\n",
    "\t\t\treturn histories\n",
    "\t\tfrontier = []\n",
    "\t\tfor history, hist_prob, _ in histories:\n",
    "\t\t\ta = self.policy(history)\n",
    "\t\t\tobservations_probs, rewards = self.transition(state, a)\n",
    "\t\t\tfor p, o, r in zip(observations_probs, self.joint_observations, rewards):\n",
    "\t\t\t\tif p == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t# the dectiger reward model is determinstic given the joint actions,\n",
    "\t\t\t\t# so no branching happens at assigning rewards\n",
    "\t\t\t\tfrontier.append((history + [a, o], hist_prob * p, r))\n",
    "\t\tself.grow_history(state, frontier, horizon-1)\n",
    "\t\tfrontier = [(h, p/self.horizon, r) for h, p, r in frontier]\n",
    "\t\tself.histories[self.states.index(state)].append(frontier)\n",
    "\n",
    "\tdef get_history_probs(self):\n",
    "\t\tres = defaultdict(list)\n",
    "\t\tfor s, values in zip(self.states, self.state_history_values):\n",
    "\t\t\tfor h, p, q in chain(*values):\n",
    "\t\t\t\tres[h].append(p)\n",
    "\n",
    "\t\tdef hist_val(P):\n",
    "\t\t\treturn sum(p * 1/2 for p in P)  # P(h) = \\sum_s P(h|s) P(s) where P(s) = 1/2\n",
    "\t\treturn {h: hist_val(P) for h, P in res.items()}\n",
    "\n",
    "\tdef get_state_given_hist_probs(self):\n",
    "\t\t# returns dictionary of P(s|h)\n",
    "\t\tres = dict()\n",
    "\t\tfor s, values in zip(self.states, self.state_history_values):\n",
    "\t\t\tfor h, p, q in chain(*values):\n",
    "\t\t\t\t# Pr(s|h) = (Pr(h|s)Pr(s)) / Pr(h)\n",
    "\t\t\t\tres[(s, h)] = (p * 1/2) / self.prob_h[h]\n",
    "\t\treturn res\n",
    "\n",
    "\tdef policy(self, history):\n",
    "\t\t# return tuple of action distribution for agents\n",
    "\t\t# action distribution: list if (action, prob) pairs\n",
    "\t\t# return list of joint action with their probabilities TODO\n",
    "\t\tif len(history)/2 < self.horizon - 1:\n",
    "\t\t\treturn ([('listen',1)], [('listen',1)])\n",
    "\n",
    "\t\tdef local_a(O):\n",
    "\t\t\tcounts = Counter(O)\n",
    "\t\t\tif counts['hear-left'] == counts['hear-right']:\n",
    "\t\t\t\treturn [('hear-left',0.5), ('hear-right',0.5)]\n",
    "\t\t\treturn [('open-right',1)] if counts.most_common(1)[0][0] == 'hear-left' else [('open-left',1)]\n",
    "\t\treturn tuple(local_a(o) for o in zip(*history[1::2]))\n",
    "\n",
    "\tdef gen_state_history_values(self, s):\n",
    "\t\t# bottom up approach to calculate all the history values\n",
    "\t\t# collected probabilities and rewards\n",
    "\t\t# state_history_values: (state, depth, state-history-value-object)\n",
    "\t\t# state-history-value-object: ([action-observations], p(h|s), Q(s,h,a))\n",
    "\t\tdef get_prob_and_expected_value(prob_and_value, back_i):\n",
    "\t\t\tp, r = zip(*prob_and_value)\n",
    "\t\t\tnormal_p = np.array(p) / sum(p)\n",
    "\t\t\treturn sum(p), sum(normal_p * np.array(r)) + back_i\n",
    "\t\tfrontiers = self.histories[s] # histories have cumulative rewards! this will cause the time penalty -2 to be added twice?\n",
    "\t\tvalues = frontiers[0] # longest set of histories, as we go back in time, we can manually add -2 to each time step, and make self.history not cumulative\n",
    "\t\t# self.history_values[s].append(values)\n",
    "\t\tlast_obs = True\n",
    "\t\tback_i = 0\n",
    "\t\twhile len(values) > 1:\n",
    "\t\t\tcollected = defaultdict(list) # groups of histories with common ancestor\n",
    "\t\t\tfor h, p, r in values:\n",
    "\t\t\t\tprev_h = tuple(h[:-1]) if last_obs else tuple(h[:-2])\n",
    "\t\t\t\tcollected[prev_h].append((p, r))\n",
    "\t\t\t\t# print(collected)\n",
    "\t\t\tlast_obs = False\n",
    "\t\t\tvalues = [(h, *get_prob_and_expected_value(p_v, back_i))\n",
    "                            for h, p_v in collected.items()]\n",
    "\t\t\tself.state_history_values[s].append(values)\n",
    "\t\t\tback_i -= 2\n",
    "\n",
    "\tdef gen_state_values(self):\n",
    "\t\tdef Qs(hs, a):\n",
    "\t\t\tfiltered_state_history_q_values = list(filter(lambda h: a in h[0], chain(*hs))) # filter for the given action\n",
    "\t\t\tif not filtered_state_history_q_values: return None\n",
    "\t\t\t_, p, r = zip(*filtered_state_history_q_values)\n",
    "\t\t\tp, r = np.array(p), np.array(r)\n",
    "\t\t\tp = p/sum(p)  # normalize\n",
    "\t\t\treturn sum(p * r)\n",
    "\t\t# need to use history values\n",
    "\t\treturn {(s, a): Qs(hs, a) for s, hs in zip(self.states, self.state_history_values) for a in self.joint_actions}\n",
    "\n",
    "\tdef gen_history_values(self):\n",
    "\t\tres = dict()\n",
    "\t\tcollected = defaultdict(list)\n",
    "\t\tfor s, values in zip(self.states, self.state_history_values):\n",
    "\t\t\tfor h, _, q in chain(*values):\n",
    "\t\t\t\t# h,a = ha[:-1], ha[-1]\n",
    "\t\t\t\tcollected[h].append((s, q))\n",
    "\t\tfor h, s_q in collected.items():\n",
    "\t\t\tS, q = zip(*s_q)\n",
    "\t\t\tp = [self.prob_s_given_h[(s, h)] for s in S]\n",
    "\t\t\tres[h] = sum(np.array(p) * np.array(q))\n",
    "\t\treturn res\n",
    "\n",
    "\tdef get_approximated_history_values(self):\n",
    "\t\t# returns \\sum_s P(s|h) Q(s, a)\n",
    "\t\tres = dict()\n",
    "\t\tcollected = defaultdict(list)\n",
    "\t\tfor s, values in zip(self.states, self.state_history_values):\n",
    "\t\t\tfor ha, _, _ in chain(*values):\n",
    "\t\t\t\th, a = ha[:-1], ha[-1]\n",
    "\t\t\t\tq = self.state_values[(s, a)]\n",
    "\t\t\t\tp = self.prob_s_given_h[(s, ha)]\n",
    "\t\t\t\tcollected[ha].append((p, q))\n",
    "\t\tfor h, p_q in collected.items():\n",
    "\t\t\tp, q = zip(*p_q)\n",
    "\t\t\tres[h] = sum(np.array(p) * np.array(q))\n",
    "\t\treturn res\n",
    "\n",
    "\tdef get_h_gradient(self, h_i):\n",
    "\t\tH = {h: p for h, p in self.prob_h.items() if self.contain(h, h_i)}\n",
    "\t\tnormalize_constraint = sum(H.values())\n",
    "\t\tfor h, p in H.items():\n",
    "\t\t\tprint(h, p/normalize_constraint, self.history_values[h])\n",
    "\t\treturn sum((p/normalize_constraint)*self.history_values[h] for h, p in H.items())\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef contain(h, h_i):\n",
    "\t\treturn len(h) == len(h_i) and all(h_t[0] == h_i_t for (h_t, h_i_t) in zip(h, h_i))\n",
    "\n",
    "\tdef get_s_gradient(self, h_i):\n",
    "\t\tapproximated_history_values = self.get_approximated_history_values()\n",
    "\t\tH = {h: p for h, p in self.prob_h.items() if self.contain(h, h_i)}\n",
    "\t\tnormalize_constraint = sum(H.values())\n",
    "\t\tfor h, p in H.items():\n",
    "\t\t\tprint(h, p/normalize_constraint, approximated_history_values[h])\n",
    "\t\treturn sum((p/normalize_constraint)*approximated_history_values[h] for h, p in H.items())\n",
    "\n",
    "\tdef list_h_i(self):\n",
    "\t\treturn [tuple(h_t[0] for h_t in h) for h in self.prob_h.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('hear-left', 'hear-right') is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/wjy268s97nj_q01sp92f7hxr0000gn/T/ipykernel_91058/568338282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecTiger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/kp/wjy268s97nj_q01sp92f7hxr0000gn/T/ipykernel_91058/3791335647.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, horizon)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;31m# history object: ([action-observations], Pr(h|s), cul_reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrow_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_history_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kp/wjy268s97nj_q01sp92f7hxr0000gn/T/ipykernel_91058/3791335647.py\u001b[0m in \u001b[0;36mgrow_history\u001b[0;34m(self, state, histories, horizon)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                 \u001b[0;31m# so no branching happens at assigning rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                 \u001b[0mfrontier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrow_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrontier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mfrontier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrontier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrontier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kp/wjy268s97nj_q01sp92f7hxr0000gn/T/ipykernel_91058/3791335647.py\u001b[0m in \u001b[0;36mgrow_history\u001b[0;34m(self, state, histories, horizon)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                 \u001b[0;31m# so no branching happens at assigning rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                 \u001b[0mfrontier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrow_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrontier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mfrontier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrontier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrontier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kp/wjy268s97nj_q01sp92f7hxr0000gn/T/ipykernel_91058/3791335647.py\u001b[0m in \u001b[0;36mgrow_history\u001b[0;34m(self, state, histories, horizon)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                         \u001b[0mobservations_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kp/wjy268s97nj_q01sp92f7hxr0000gn/T/ipykernel_91058/3791335647.py\u001b[0m in \u001b[0;36mtransition\u001b[0;34m(self, state, actions)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions2a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \t\tobs_probs = [self.d.O[(*a, s, *self.obs2o(obs))]\n",
      "\u001b[0;32m/var/folders/kp/wjy268s97nj_q01sp92f7hxr0000gn/T/ipykernel_91058/3791335647.py\u001b[0m in \u001b[0;36mactions2a\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mactions2a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mobs2o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kp/wjy268s97nj_q01sp92f7hxr0000gn/T/ipykernel_91058/3791335647.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mactions2a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mobs2o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('hear-left', 'hear-right') is not in list"
     ]
    }
   ],
   "source": [
    "d = DecTiger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('listen', 'listen'), ('hear-left', 'hear-left'), ('listen', 'listen')),\n",
       "  0.24083333333333332,\n",
       "  18.0),\n",
       " ((('listen', 'listen'), ('hear-left', 'hear-right'), ('listen', 'listen')),\n",
       "  0.0425,\n",
       "  -101.99999999999999),\n",
       " ((('listen', 'listen'), ('hear-right', 'hear-left'), ('listen', 'listen')),\n",
       "  0.0425,\n",
       "  -101.99999999999999),\n",
       " ((('listen', 'listen'), ('hear-right', 'hear-right'), ('listen', 'listen')),\n",
       "  0.007499999999999999,\n",
       "  -52.00000000000001)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.state_history_values[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('tiger-left', ('listen', 'listen')): -14.841666666666669,\n",
       " ('tiger-left', ('listen', 'open-left')): None,\n",
       " ('tiger-left', ('listen', 'open-right')): None,\n",
       " ('tiger-left', ('open-left', 'listen')): None,\n",
       " ('tiger-left', ('open-left', 'open-left')): -50.00000000000001,\n",
       " ('tiger-left', ('open-left', 'open-right')): -99.99999999999999,\n",
       " ('tiger-left', ('open-right', 'listen')): None,\n",
       " ('tiger-left', ('open-right', 'open-left')): -99.99999999999999,\n",
       " ('tiger-left', ('open-right', 'open-right')): 20.0,\n",
       " ('tiger-right', ('listen', 'listen')): -14.841666666666665,\n",
       " ('tiger-right', ('listen', 'open-left')): None,\n",
       " ('tiger-right', ('listen', 'open-right')): None,\n",
       " ('tiger-right', ('open-left', 'listen')): None,\n",
       " ('tiger-right', ('open-left', 'open-left')): 20.0,\n",
       " ('tiger-right', ('open-left', 'open-right')): -100.00000000000001,\n",
       " ('tiger-right', ('open-right', 'listen')): None,\n",
       " ('tiger-right', ('open-right', 'open-left')): -100.00000000000001,\n",
       " ('tiger-right', ('open-right', 'open-right')): -50.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.gen_state_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "332e788ca0ee159753d6a3b1af8ac999dc5ddcdb4aaa2ea64c47d77127898cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
